---
title: "BPR: Bayesian Personalized Ranking from Implicit Feedback"
date: 2024-11-18
categories: [Paper_review,EN]
tags: [bpr loss, implicit feedback, ranking]
toc: true
toc_label: "Table of contents"
toc_icon: "cog"
math: true
---

**Disclaimer**
This blog post is a summary of what I understood reading the paper. This is for personal use and might contain wrong information.

---

# Abstract

There are methods like MF or kNN for recommending items with implicit feedback, but none of them
directly consider ranking. The paper introduces BPR-OPT, which is a loss function(criterion),for 
personalized ranking, derived from Bayesian. The paper also introduces a learning algorithm for the 
criterion and compared to MF and adaptive kNN.

# Introduction

Preferences of users about items are inferred from the user's past interaction with the items, like
purchasing history and click history. Explicit feedbacks would, in ideal situations, reveal 
full extent of users' preferences, but it is hard to gather them since it takes additional actions
to leave explicit feedbacks. Hence, implicit feedbacks are often invesitgated in recommender systems
due to its ease of collection of data. 

# Related Work

the related work part is ommitted not being related to BPR. It introduces MF and kNN.

# Personalized ranking

There have been efforts on learning to rank with non CF models, but their purpose is not for personal
recommendation. One thing to note about implicit feedback is that they are all positive feedbacks.
They are all labelled as 1(pointwise models), whereas in explicit feedback we tend to know whether
a feedback is negative or positive, in terms of ratings.(e.g. 1 out of 5 would be negative and 5 out of 
5 would be positive). So, the items that have not yet been interacted with would be a mixture of real
negative feedback and missing values, that users have not yet discovered. 

Let $U$ refer to the set of all users and $I$ the set of all items. So, we have a scenario where each user
can interact with all the items in $I$. And, the recommender system needs to provide the user with a 
pairwise personalized ranking, where ranking has to meet the properties:

![image](https://github.com/user-attachments/assets/8d936cba-1a2d-4457-9945-f2b485562755)

In implicit feedback where we need to rank items that have yet to be observed, the usual approach is 
to predict a personalized score $\hat{x}_{ui}$ to reflect the preference of the user for the item and 
sort the scores. So, all the negative feedbacks are inputted with a label 0 and they are the items that 
the system needs to rank. So, the methods cannot properly rank them due to its limitation.

With BPR, item pairs are constructed with the assumption that observed items should be preferred better
than unobserved items. For pairs of items that are both observed, we cannot rank them. And, the same logic
applies to pairs of negative feedback. Therefore, training data is generated by:

![image](https://github.com/user-attachments/assets/4cc80b8c-5c1b-41c5-a1e5-e657e52710d5)

By the formula above, the test data will consist of missing values that we need to rank in between. So, 
the training data will be disjoint from the test data.

# BPR

![image](https://github.com/user-attachments/assets/35d56a68-d9d4-4b01-a950-6cb99e060c52)


The objective of BPR is to maximize the posterior probability above where $\theta$ represent the parameter vector.
(e.g. latent vector parameters). It is assumed that all users act independent of each other and 
the ordering of each pair of items for a user is independent of the ordering of other pairs. With the 
assumptions, the conditional function can be rearranged into a product of single densities.

![image](https://github.com/user-attachments/assets/6c5d158a-20cd-4f3f-b641-3b4f3d9cbdc1)

To fulfill the properties mentioned in ranking, the individual probability that a user prefers item i over 
item j can be written as:

![image](https://github.com/user-attachments/assets/cfa3abd6-6b55-4348-8340-4814dd22a6c0)

and $\hat{x}_{uij}$ is a function of the model parameter vector $theta$ and can be applied to models
like MF and adaptive kNN which predicts $\hat{x}_{uij}$.
